- 驱动层

  - epoll 跟 poll 一样，区别在与其内核层和用户层

- 用户层

```c
int main()
{
    int fd = open("/dev/epoll_demo", O_RDONLY | O_NONBLOCK);
    if (fd < 0) {
        perror("open");
        return -1;
    }

    int epfd = epoll_create1(0);

    struct epoll_event ev;
    ev.events = EPOLLIN;
    ev.data.fd = fd;

    epoll_ctl(epfd, EPOLL_CTL_ADD, fd, &ev);

    printf("wait for event...\n");

    while (1) {
        struct epoll_event events[1];
        int nfds = epoll_wait(epfd, events, 1, -1);

        if (nfds > 0) {
            printf("epoll notified\n");

            char buf[8];
            int ret = read(fd, buf, sizeof(buf));
            printf("read ret = %d, char=%c\n", ret, buf[0]);
        }
    }

    close(epfd);
    close(fd);
    return 0;
}
```

- 实现原理

  - int epoll_ctl(int epfd, int op, int fd, struct epoll_event \*event);

    ```c
    SYSCALL_DEFINE4(epoll_ctl, int, epfd, int, op, int, fd, struct epoll_event __user *, event) {
    	struct epitem *epi;
    	struct fd f, tf;
    	struct eventpoll *ep;

    	f = fdget(epfd);
    	tf = fdget(fd);

    	ep = f.file->private_data;
    	epi = ep_find(ep, tf.file, fd);

    	......
    	switch (op) {
    		case EPOLL_CTL_ADD:
    			if (!epi) {
    				epds.events |= EPOLLERR | EPOLLHUP;
    				error = ep_insert(ep, &epds, tf.file, fd, full_check);
    			} else
    				error = -EEXIST;
    			if (full_check)
    				clear_tfile_check_list();
    			break;
    	}
    }
    static int ep_insert(struct eventpoll *ep, const struct epoll_event *event,
    			struct file *tfile, int fd, int full_check)
    {
    	int error, pwake = 0;
    	__poll_t revents;
    	long user_watches;
    	struct epitem *epi;
    	struct ep_pqueue epq;

    	lockdep_assert_irqs_enabled();

    	user_watches = atomic_long_read(&ep->user->epoll_watches);
    	if (unlikely(user_watches >= max_user_watches))
    		return -ENOSPC;
    	if (!(epi = kmem_cache_alloc(epi_cache, GFP_KERNEL)))
    		return -ENOMEM;

    	/* Item initialization follow here ... */
    	INIT_LIST_HEAD(&epi->rdllink);
    	INIT_LIST_HEAD(&epi->fllink);
    	INIT_LIST_HEAD(&epi->pwqlist);
    	epi->ep = ep;
    	ep_set_ffd(&epi->ffd, tfile, fd);
    	epi->event = *event;
    	epi->nwait = 0;
    	epi->next = EP_UNACTIVE_PTR;
    	if (epi->event.events & EPOLLWAKEUP) {
    		error = ep_create_wakeup_source(epi);
    		if (error)
    			goto error_create_wakeup_source;
    	} else {
    		RCU_INIT_POINTER(epi->ws, NULL);
    	}

    	/* Initialize the poll table using the queue callback */
    	epq.epi = epi;
    	init_poll_funcptr(&epq.pt, ep_ptable_queue_proc);
    	......
    }
    static inline void poll_wait(struct file * filp, wait_queue_head_t * wait_address, poll_table *p)
    {
    	if (p && p->_qproc && wait_address)
    		p->_qproc(filp, wait_address, p);
    }
    ```

  - ep_ptable_queue_proc

    ```c
    static void ep_ptable_queue_proc(struct file *file, wait_queue_head_t *whead,
    		 poll_table *pt)
    {
    	struct epitem *epi = ep_item_from_epqueue(pt);
    	struct eppoll_entry *pwq;

    	if (epi->nwait >= 0 && (pwq = kmem_cache_alloc(pwq_cache, GFP_KERNEL))) {
    		init_waitqueue_func_entry(&pwq->wait, ep_poll_callback);
    		pwq->whead = whead;
    		pwq->base = epi;
    		if (epi->event.events & EPOLLEXCLUSIVE)
    			add_wait_queue_exclusive(whead, &pwq->wait);
    		else
    			add_wait_queue(whead, &pwq->wait);
    		list_add_tail(&pwq->llink, &epi->pwqlist);
    		epi->nwait++;
    	} else {
    		/* We have to signal that an error occurred */
    		epi->nwait = -1;
    	}
    }
    static inline void
    init_waitqueue_func_entry(struct wait_queue_entry *wq_entry, wait_queue_func_t func)
    {
    	wq_entry->flags		= 0;
    	wq_entry->private	= NULL;
    	wq_entry->func		= func;
    }
    static int ep_poll_callback(wait_queue_entry_t *wait, unsigned mode, int sync, void *key)
    {
    	int pwake = 0;
    	unsigned long flags;
    	struct epitem *epi = ep_item_from_wait(wait);
    	struct eventpoll *ep = epi->ep;
    	__poll_t pollflags = key_to_poll(key);
    	int ewake = 0;

    	spin_lock_irqsave(&ep->wq.lock, flags);

    	ep_set_busy_poll_napi_id(epi);

    	/*
    	* If the event mask does not contain any poll(2) event, we consider the
    	* descriptor to be disabled. This condition is likely the effect of the
    	* EPOLLONESHOT bit that disables the descriptor when an event is received,
    	* until the next EPOLL_CTL_MOD will be issued.
    	*/
    	if (!(epi->event.events & ~EP_PRIVATE_BITS))
    		goto out_unlock;

    	/*
    	* Check the events coming with the callback. At this stage, not
    	* every device reports the events in the "key" parameter of the
    	* callback. We need to be able to handle both cases here, hence the
    	* test for "key" != NULL before the event match test.
    	*/
    	if (pollflags && !(pollflags & epi->event.events))
    		goto out_unlock;

    	/*
    	* If we are transferring events to userspace, we can hold no locks
    	* (because we're accessing user memory, and because of linux f_op->poll()
    	* semantics). All the events that happen during that period of time are
    	* chained in ep->ovflist and requeued later on.
    	*/
    	if (READ_ONCE(ep->ovflist) != EP_UNACTIVE_PTR) {
    		if (epi->next == EP_UNACTIVE_PTR) {
    			epi->next = READ_ONCE(ep->ovflist);
    			WRITE_ONCE(ep->ovflist, epi);
    			if (epi->ws) {
    				/*
    				* Activate ep->ws since epi->ws may get
    				* deactivated at any time.
    				*/
    				__pm_stay_awake(ep->ws);
    			}

    		}
    		goto out_unlock;
    	}

    	/* If this file is already in the ready list we exit soon */
    	if (!ep_is_linked(epi)) {
    		list_add_tail(&epi->rdllink, &ep->rdllist);
    		ep_pm_stay_awake_rcu(epi);
    	}

    	/*
    	* Wake up ( if active ) both the eventpoll wait list and the ->poll()
    	* wait list.
    	*/
    	if (waitqueue_active(&ep->wq)) {
    		if ((epi->event.events & EPOLLEXCLUSIVE) &&
    					!(pollflags & POLLFREE)) {
    			switch (pollflags & EPOLLINOUT_BITS) {
    			case EPOLLIN:
    				if (epi->event.events & EPOLLIN)
    					ewake = 1;
    				break;
    			case EPOLLOUT:
    				if (epi->event.events & EPOLLOUT)
    					ewake = 1;
    				break;
    			case 0:
    				ewake = 1;
    				break;
    			}
    		}
    		wake_up_locked(&ep->wq);
    	}
    	if (waitqueue_active(&ep->poll_wait))
    		pwake++;

    out_unlock:
    	spin_unlock_irqrestore(&ep->wq.lock, flags);

    	/* We have to call this outside the lock */
    	if (pwake)
    		ep_poll_safewake(&ep->poll_wait);

    	if (!(epi->event.events & EPOLLEXCLUSIVE))
    		ewake = 1;

    	if (pollflags & POLLFREE) {
    		/*
    		* If we race with ep_remove_wait_queue() it can miss
    		* ->whead = NULL and do another remove_wait_queue() after
    		* us, so we can't use __remove_wait_queue().
    		*/
    		list_del_init(&wait->entry);
    		/*
    		* ->whead != NULL protects us from the race with ep_free()
    		* or ep_remove(), ep_remove_wait_queue() takes whead->lock
    		* held by the caller. Once we nullify it, nothing protects
    		* ep/epi or even wait.
    		*/
    		smp_store_release(&ep_pwq_from_wait(wait)->whead, NULL);
    	}

    	return ewake;
    }
    ```

  * epoll_wait 直接从 ready-list 拷贝返回（避免每次遍历大量 fd）

static \_\_poll_t (const struct epitem *epi, poll_table *pt,
int depth)
{
struct eventpoll \*ep;
bool locked;

    pt->_key = epi->event.events;
    if (!is_file_epoll(epi->ffd.file))
    	return vfs_poll(epi->ffd.file, pt) & epi->event.events;

    ep = epi->ffd.file->private_data;
    poll_wait(epi->ffd.file, &ep->poll_wait, pt);
    locked = pt && (pt->_qproc == ep_ptable_queue_proc);

    return ep_scan_ready_list(epi->ffd.file->private_data,
    			  ep_read_events_proc, &depth, depth,
    			  locked) & epi->event.events;

}
